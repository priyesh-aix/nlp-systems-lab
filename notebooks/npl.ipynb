{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2089c690-d44e-43b8-9728-59017cd75955",
   "metadata": {},
   "source": [
    "# üßπ NLP Text Cleaning Pipeline (Visualization)\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[üìÇ Read Raw Text] --> B[‚úÇÔ∏è Tokenization<br/>Sentence + Word]\n",
    "    B --> C[üîΩ Lowercasing]\n",
    "    C --> D[üö´ Stopword Removal]\n",
    "    D --> E[‚úÇÔ∏è Punctuation Removal]\n",
    "    E --> F[üå± Stemming]\n",
    "    E --> G[üå≥ Lemmatization]\n",
    "    F --> H[üîó N-gram Generation<br/>Unigram, Bigram, Trigram]\n",
    "    G --> H\n",
    "    H --> I[üìä Feature Extraction<br/>BoW, TF-IDF, Embeddings]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855823f-ba42-4382-818d-3c601c9e4008",
   "metadata": {},
   "source": [
    "## 1. Read Raw Text & Lowercasing  \n",
    "\n",
    "üìÇ **Load the text file** ‚Üí bring data into memory.  \n",
    "‚úÇÔ∏è **Split** into sentences and words for structured processing.  \n",
    "üîΩ **Convert everything to lowercase** ‚Üí ensures consistency (*Apple = apple*).  \n",
    "\n",
    "‚û°Ô∏è This is the foundation step: get your raw input into a clean, uniform format before applying deeper NLP transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c452b576-3154-4093-a9ae-e7443d50b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "with open(\"../data/sample.txt\", \"r+\") as f:\n",
    "    for line in f.readlines():\n",
    "        lines.append(line.lower().strip().replace(\".\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ec6a3e5-c768-4ac1-8132-0bd967b2d7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in july 2023, google announced a $2 billion investment in bengaluru to expand its cloud data centers', 'sundar pichai emphasized that india is one of the fastest-growing digital markets', 'meanwhile, microsoft signed a partnership with infosys to accelerate ai adoption across asia', 'apple released the iphone 15 in september 2023, and tim cook visited mumbai for the launch', 'according to the times of india, the reserve bank of india may introduce new regulations for digital payments by early 2024', '']\n"
     ]
    }
   ],
   "source": [
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b773930-1aec-4ac4-8d8f-d00101ba387e",
   "metadata": {},
   "source": [
    "## üîπ 2. Tokenization  \n",
    "- üìù **Manual Tokenization** ‚Üí `split()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a5d509-ae92-4e9e-88a5-5924f9131d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [word for line in lines for word in line.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37abba12-dd4a-47a6-984c-2e365e244c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'july', '2023,', 'google', 'announced', 'a', '$2', 'billion', 'investment', 'in', 'bengaluru', 'to', 'expand', 'its', 'cloud', 'data', 'centers', 'sundar', 'pichai', 'emphasized', 'that', 'india', 'is', 'one', 'of', 'the', 'fastest-growing', 'digital', 'markets', 'meanwhile,', 'microsoft', 'signed', 'a', 'partnership', 'with', 'infosys', 'to', 'accelerate', 'ai', 'adoption', 'across', 'asia', 'apple', 'released', 'the', 'iphone', '15', 'in', 'september', '2023,', 'and', 'tim', 'cook', 'visited', 'mumbai', 'for', 'the', 'launch', 'according', 'to', 'the', 'times', 'of', 'india,', 'the', 'reserve', 'bank', 'of', 'india', 'may', 'introduce', 'new', 'regulations', 'for', 'digital', 'payments', 'by', 'early', '2024']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5708024b-aa12-4df7-a3c3-0b1e959a5a17",
   "metadata": {},
   "source": [
    "## üîπ 3. Stopword Removal  \n",
    "üö´ Remove high-frequency, low-information words (*the, is, at, on, and‚Ä¶*).  \n",
    "‚û°Ô∏è Keeps the focus on **meaningful tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454a00d9-9ba9-4f0f-a3f9-0fafa62178f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/priyesh/Documents/workspace/venv/jupyterlab/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/priyesh/Documents/workspace/venv/jupyterlab/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /home/priyesh/Documents/workspace/venv/jupyterlab/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/priyesh/Documents/workspace/venv/jupyterlab/lib/python3.12/site-packages (from nltk) (2025.8.29)\n",
      "Requirement already satisfied: tqdm in /home/priyesh/Documents/workspace/venv/jupyterlab/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fd950e0-2358-4ae2-86e6-6df3f7a0a00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/priyesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd0e7be0-e90b-4075-964c-bd2187d2c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092e30d5-6deb-46ab-b4aa-614623c5292a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04286a9-3096-438d-b88c-cb28d745a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stopwords = [word for word in cleaned if word not in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62da6b3-4058-44fc-8f2f-ed7053edded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['july', '2023,', 'google', 'announced', '$2', 'billion', 'investment', 'bengaluru', 'expand', 'cloud', 'data', 'centers', 'sundar', 'pichai', 'emphasized', 'india', 'one', 'fastest-growing', 'digital', 'markets', 'meanwhile,', 'microsoft', 'signed', 'partnership', 'infosys', 'accelerate', 'ai', 'adoption', 'across', 'asia', 'apple', 'released', 'iphone', '15', 'september', '2023,', 'tim', 'cook', 'visited', 'mumbai', 'launch', 'according', 'times', 'india,', 'reserve', 'bank', 'india', 'may', 'introduce', 'new', 'regulations', 'digital', 'payments', 'early', '2024']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d87896-d5f5-4d41-b5c0-68ed4a0ad05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'july', '2023,', 'google', 'announced', 'a', '$2', 'billion', 'investment', 'in', 'bengaluru', 'to', 'expand', 'its', 'cloud', 'data', 'centers', 'sundar', 'pichai', 'emphasized', 'that', 'india', 'is', 'one', 'of', 'the', 'fastest-growing', 'digital', 'markets', 'meanwhile,', 'microsoft', 'signed', 'a', 'partnership', 'with', 'infosys', 'to', 'accelerate', 'ai', 'adoption', 'across', 'asia', 'apple', 'released', 'the', 'iphone', '15', 'in', 'september', '2023,', 'and', 'tim', 'cook', 'visited', 'mumbai', 'for', 'the', 'launch', 'according', 'to', 'the', 'times', 'of', 'india,', 'the', 'reserve', 'bank', 'of', 'india', 'may', 'introduce', 'new', 'regulations', 'for', 'digital', 'payments', 'by', 'early', '2024']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51b482db-bf40-43ad-bf72-5a53b446cbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f24cc7-40de-4352-b38a-377d0b24c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_reviews = ['sam was a great help to me in the store', \n",
    "                    'the cashier was very rude to me, I think her name was eleanor', \n",
    "                    'amazing work from sadeen!', \n",
    "                    'sarah was able to help me find the items i needed quickly', \n",
    "                    'lucy is such a great addition to the team', \n",
    "                    'great service from sara she found me what i wanted'\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24688e59-84e4-4250-b0ea-19de1d27d5f6",
   "metadata": {},
   "source": [
    "## üîπ 4. Punctuation Removal  \n",
    "‚úÇÔ∏è Remove punctuation marks (`.,!?;:\"()[]‚Ä¶`).  \n",
    "‚û°Ô∏è Reduces noise for models like BoW, TF-IDF, or embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28b33f2-b438-4840-acc8-85b8cab42c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9266b839-f68c-46e0-8ae1-c709d34c387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations from list of sentences\n",
    "punc_cleaned = [re.sub(r\"[^\\w\\s]\", \"\", review) for review in customer_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e86202-8af4-4678-9c50-c6697a02590d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sam was a great help to me in the store',\n",
       " 'the cashier was very rude to me I think her name was eleanor',\n",
       " 'amazing work from sadeen',\n",
       " 'sarah was able to help me find the items i needed quickly',\n",
       " 'lucy is such a great addition to the team',\n",
       " 'great service from sara she found me what i wanted']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0942014f-bf76-47ae-95fe-a350e278b5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sam was a great help to me in the store',\n",
       " 'the cashier was very rude to me, I think her name was eleanor',\n",
       " 'amazing work from sadeen!',\n",
       " 'sarah was able to help me find the items i needed quickly',\n",
       " 'lucy is such a great addition to the team',\n",
       " 'great service from sara she found me what i wanted']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9b2fc1-b0c3-4c14-8fe0-9d9f7624a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations from list of tokens\n",
    "stopwords_list_punc_cleaned = [re.sub(r\"[^\\w\\s]\", \"\", word) for word in stopwords_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e2b3e9-7d51-4246-8de8-80c85b89dbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'arent', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'couldnt', 'd', 'did', 'didn', 'didnt', 'do', 'does', 'doesn', 'doesnt', 'doing', 'don', 'dont', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'hed', 'hell', 'her', 'here', 'hers', 'herself', 'hes', 'him', 'himself', 'his', 'how', 'i', 'id', 'if', 'ill', 'im', 'in', 'into', 'is', 'isn', 'isnt', 'it', 'itd', 'itll', 'its', 'its', 'itself', 'ive', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'mightnt', 'more', 'most', 'mustn', 'mustnt', 'my', 'myself', 'needn', 'neednt', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'shant', 'she', 'shed', 'shell', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'so', 'some', 'such', 't', 'than', 'that', 'thatll', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'theyd', 'theyll', 'theyre', 'theyve', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', 'wasnt', 'we', 'wed', 'well', 'were', 'were', 'weren', 'werent', 'weve', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'wont', 'wouldn', 'wouldnt', 'y', 'you', 'youd', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_list_punc_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "193525cf-b223-4a0b-a8fa-f11c3219e66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6004db42-2cb2-4901-a295-b9f8108feed9",
   "metadata": {},
   "source": [
    "## üîπ 5. Tokenization  \n",
    "- üìù **Sentence Tokenization** ‚Üí `sent_tokenize()`  \n",
    "- üî§ **Word Tokenization** ‚Üí `word_tokenize()`  \n",
    "\n",
    "‚û°Ô∏è Converts raw text into sentences and words (tokens). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "448ccb85-10cd-48d6-95ac-73e14fa6cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/priyesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/priyesh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25369b26-8997-4c31-9c15-0dca52208a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "para = \"\"\"In September 2023, Priyesh moved to Bengaluru to join an AI research lab. \n",
    "He said, \"I‚Äôm excited to explore natural language processing, deep learning, and data engineering!\"\n",
    "During weekends, he often visits bookstores, drinks coffee at Church Street, and writes blogs about technology.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf2c096f-2ae2-4694-8a38-cea4336b24eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenizer\n",
    "tokenize_sentence = sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "969bde05-3ade-4688-be0e-9878ba928db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In September 2023, Priyesh moved to Bengaluru to join an AI research lab.',\n",
       " 'He said, \"I‚Äôm excited to explore natural language processing, deep learning, and data engineering!\"',\n",
       " 'During weekends, he often visits bookstores, drinks coffee at Church Street, and writes blogs about technology.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "720904c6-412a-4f4a-bce4-245b823bb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizer\n",
    "tokenize_words = word_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7eac94a-08b9-4ccd-b376-0ff9ff0e4749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'September', '2023', ',', 'Priyesh', 'moved', 'to', 'Bengaluru', 'to', 'join', 'an', 'AI', 'research', 'lab', '.', 'He', 'said', ',', '``', 'I', '‚Äô', 'm', 'excited', 'to', 'explore', 'natural', 'language', 'processing', ',', 'deep', 'learning', ',', 'and', 'data', 'engineering', '!', \"''\", 'During', 'weekends', ',', 'he', 'often', 'visits', 'bookstores', ',', 'drinks', 'coffee', 'at', 'Church', 'Street', ',', 'and', 'writes', 'blogs', 'about', 'technology', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fd17e-76c2-415a-8e3b-6c84bfebe1f4",
   "metadata": {},
   "source": [
    "## üîπ 6. Normalization  \n",
    "- üå± **Stemming** ‚Üí crude root form (*studies ‚Üí studi*).  \n",
    "- üå≥ **Lemmatization** ‚Üí dictionary form (*running ‚Üí run*).\n",
    "‚û°Ô∏è Shrinks vocabulary & improves generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f26dfe94-6f97-4333-9db4-c10fdb56b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "800bedc1-1fdd-4473-b409-bcf8b37e7b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55fdcb2d-7873-4571-b2aa-36efc726e74b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in september 2023, priyesh moved to bengaluru to join an ai research lab. \\nhe said, \"i‚Äôm excited to explore natural language processing, deep learning, and data engineering!\"\\nduring weekends, he often visits bookstores, drinks coffee at church street, and writes blogs about technology.\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pps.stem(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fedabad6-4050-413e-9e85-3a87a3a6ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming\n",
    "tokenize_stemmed = [pps.stem(word) for word in stopwords_list_punc_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e0e41bb-bff7-48fd-8ebb-544a6446c41c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'abov', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'ani', 'are', 'aren', 'arent', 'as', 'at', 'be', 'becaus', 'been', 'befor', 'be', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'couldnt', 'd', 'did', 'didn', 'didnt', 'do', 'doe', 'doesn', 'doesnt', 'do', 'don', 'dont', 'down', 'dure', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'hadnt', 'ha', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'have', 'he', 'hed', 'hell', 'her', 'here', 'her', 'herself', 'he', 'him', 'himself', 'hi', 'how', 'i', 'id', 'if', 'ill', 'im', 'in', 'into', 'is', 'isn', 'isnt', 'it', 'itd', 'itll', 'it', 'it', 'itself', 'ive', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'mightnt', 'more', 'most', 'mustn', 'mustnt', 'my', 'myself', 'needn', 'neednt', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'onc', 'onli', 'or', 'other', 'our', 'our', 'ourselv', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'shant', 'she', 'shed', 'shell', 'she', 'should', 'shouldn', 'shouldnt', 'shouldv', 'so', 'some', 'such', 't', 'than', 'that', 'thatll', 'the', 'their', 'their', 'them', 'themselv', 'then', 'there', 'these', 'they', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'veri', 'wa', 'wasn', 'wasnt', 'we', 'wed', 'well', 'were', 'were', 'weren', 'werent', 'weve', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'whi', 'will', 'with', 'won', 'wont', 'wouldn', 'wouldnt', 'y', 'you', 'youd', 'youll', 'your', 'your', 'your', 'yourself', 'yourselv', 'youv']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7381a7ff-c360-4af6-8362-8f01f7cf91f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting ‚Üí connect\n",
      "connected ‚Üí connect\n",
      "connectivity ‚Üí connect\n",
      "connect ‚Üí connect\n",
      "connects ‚Üí connect\n"
     ]
    }
   ],
   "source": [
    "tokens_connect = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "for token in tokens_connect:\n",
    "    print(token, \"‚Üí\", pps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55058d2f-30ea-4190-b906-12492868b801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned ‚Üí learn\n",
      "learning ‚Üí learn\n",
      "learn ‚Üí learn\n",
      "learns ‚Üí learn\n",
      "learner ‚Üí learner\n",
      "learners ‚Üí learner\n"
     ]
    }
   ],
   "source": [
    "tokens_learn = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "for token in tokens_learn:\n",
    "    print(token, \"‚Üí\", pps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9020b1a6-6b53-4aef-8e04-5ec2a31b94f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes ‚Üí like\n",
      "better ‚Üí better\n",
      "worse ‚Üí wors\n"
     ]
    }
   ],
   "source": [
    "tokens_misc = ['likes', 'better', 'worse']\n",
    "for token in tokens_misc:\n",
    "    print(token, \"‚Üí\", pps.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "053f88d4-1017-493e-8c19-20b4f651c415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/priyesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/priyesh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc79e9ec-4a3b-41e2-99b3-a3db50bcb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ada7be4d-68a4-47c8-9916-61cfce5d4016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting ‚Üí connecting\n",
      "connected ‚Üí connected\n",
      "connectivity ‚Üí connectivity\n",
      "connect ‚Üí connect\n",
      "connects ‚Üí connects\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "tokens_connect = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "for token in tokens_connect:\n",
    "    print(token, \"‚Üí\", lem.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df36b43e-4247-44e0-9f2d-7d899c4d09f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned ‚Üí learned\n",
      "learning ‚Üí learning\n",
      "learn ‚Üí learn\n",
      "learns ‚Üí learns\n",
      "learner ‚Üí learner\n",
      "learners ‚Üí learner\n"
     ]
    }
   ],
   "source": [
    "tokens_learn = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "for token in tokens_learn:\n",
    "    print(token, \"‚Üí\", lem.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbb80996-468a-4d99-8c9c-7cfe28b54f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes ‚Üí like\n",
      "better ‚Üí better\n",
      "worse ‚Üí worse\n"
     ]
    }
   ],
   "source": [
    "tokens_misc = ['likes', 'better', 'worse']\n",
    "for token in tokens_misc:\n",
    "    print(token, \"‚Üí\", lem.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a33e096-0310-44bb-99cc-ff3b254c2275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'abov', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'ani', 'are', 'aren', 'arent', 'as', 'at', 'be', 'becaus', 'been', 'befor', 'be', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'couldnt', 'd', 'did', 'didn', 'didnt', 'do', 'doe', 'doesn', 'doesnt', 'do', 'don', 'dont', 'down', 'dure', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'hadnt', 'ha', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'have', 'he', 'hed', 'hell', 'her', 'here', 'her', 'herself', 'he', 'him', 'himself', 'hi', 'how', 'i', 'id', 'if', 'ill', 'im', 'in', 'into', 'is', 'isn', 'isnt', 'it', 'itd', 'itll', 'it', 'it', 'itself', 'ive', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'mightnt', 'more', 'most', 'mustn', 'mustnt', 'my', 'myself', 'needn', 'neednt', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'onc', 'onli', 'or', 'other', 'our', 'our', 'ourselv', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'shant', 'she', 'shed', 'shell', 'she', 'should', 'shouldn', 'shouldnt', 'shouldv', 'so', 'some', 'such', 't', 'than', 'that', 'thatll', 'the', 'their', 'their', 'them', 'themselv', 'then', 'there', 'these', 'they', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'veri', 'wa', 'wasn', 'wasnt', 'we', 'wed', 'well', 'were', 'were', 'weren', 'werent', 'weve', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'whi', 'will', 'with', 'won', 'wont', 'wouldn', 'wouldnt', 'y', 'you', 'youd', 'youll', 'your', 'your', 'your', 'yourself', 'yourselv', 'youv']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae659440-e49f-4a57-aa2b-aa61f374ab84",
   "metadata": {},
   "source": [
    "## üîπ 8. N-gram Generation  \n",
    "- **Unigram** ‚Üí single words (*‚Äúdata‚Äù*).  \n",
    "- **Bigram** ‚Üí word pairs (*‚Äúdata science‚Äù*).  \n",
    "- **Trigram** ‚Üí three-word phrases (*‚Äúnew york city‚Äù*).  \n",
    "\n",
    "‚û°Ô∏è Captures short-range context useful in tasks like sentiment analysis (*‚Äúnot good‚Äù*).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a815921-8a49-40d7-8b7b-6522909584b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b003a7c9-363b-43ad-bc9c-211d67255274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('it',), 3), (('your',), 3), (('be',), 2), (('do',), 2), (('have',), 2)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigrams = list(ngrams(tokenize_stemmed, 1))\n",
    "unigrams_counter = Counter(unigrams)\n",
    "unigrams_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0332c0b8-ddec-4cb8-bdcf-4f4eaf5e6474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('your', 'your'), 2),\n",
       " (('a', 'about'), 1),\n",
       " (('about', 'abov'), 1),\n",
       " (('abov', 'after'), 1),\n",
       " (('after', 'again'), 1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = list(ngrams(tokenize_stemmed, 2))\n",
    "bigrams_counter = Counter(bigrams)\n",
    "bigrams_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3c6078b3-4422-4a12-9148-1588ff1b5fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('a', 'about', 'abov'), 1),\n",
       " (('about', 'abov', 'after'), 1),\n",
       " (('abov', 'after', 'again'), 1),\n",
       " (('after', 'again', 'against'), 1),\n",
       " (('again', 'against', 'ain'), 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = list(ngrams(tokenize_stemmed, 3))\n",
    "trigrams_counter = Counter(trigrams)\n",
    "trigrams_counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc1a3f-a74e-45a7-bb77-71a4d3a28498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
