{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7907e601-6ac1-40a3-97ef-140e437d087b",
   "metadata": {},
   "source": [
    "# ğŸš€ Introduction to the Course\n",
    "\n",
    "Hello and welcome to this **Introduction to Natural Language Processing (NLP)** course.  \n",
    "We will be delving into the exciting field of NLP and exploring techniques that enable computers to **understand, generate, and classify human language**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š What You Need\n",
    "- âœ… Basic Python skills  \n",
    "- âœ… Some familiarity with machine learning  \n",
    "- âŒ No prior NLP knowledge required  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—‚ï¸ Course Roadmap\n",
    "\n",
    "1. **Introduction to NLP**  \n",
    "   - What NLP means  \n",
    "   - Everyday applications of NLP  \n",
    "\n",
    "2. **ğŸ”§ Text Pre-processing**  \n",
    "   - Fundamental step in NLP  \n",
    "   - Practical exercises included  \n",
    "\n",
    "3. **ğŸ§© Key Component Extraction**  \n",
    "   - Parts of Speech (POS) Tagging  \n",
    "   - Named Entity Recognition (NER)  \n",
    "\n",
    "4. **ğŸ’¬ Sentiment Analysis**  \n",
    "   - Understanding emotions in text  \n",
    "\n",
    "5. **ğŸ”¢ Text Vectorization**  \n",
    "   - Preparing text data for machine learning  \n",
    "\n",
    "6. **ğŸ“Š Advanced Topics**  \n",
    "   - Topic modeling  \n",
    "   - Building a custom text classifier  \n",
    "\n",
    "7. **ğŸ¢ Case Study (Section A)**  \n",
    "   - End-to-end business problem solution  \n",
    "   - Portfolio-ready Jupyter Notebook  \n",
    "\n",
    "8. **ğŸ”® Future of NLP**  \n",
    "   - Deep Learning in NLP  \n",
    "   - Large Language Models (LLMs)  \n",
    "   - Trends & future directions  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‘©â€ğŸ« Instructor Introduction\n",
    "\n",
    "**Lauren Newbold**  \n",
    "- Data Scientist with experience in **large organizations and startups**  \n",
    "- Built a **custom NLP text classifier** for interview data in developing communities  \n",
    "- Speaker at **international conferences** on NLP in developing countries  \n",
    "- Passionate about **teaching and making data exciting**  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ¨ Key Takeaways\n",
    "- ğŸ¤– NLP enables computers to **understand, generate, and classify** human language.  \n",
    "- ğŸ Requires **basic Python skills** and some ML knowledge â€” no prior NLP experience.  \n",
    "- ğŸ“˜ Covers: preprocessing, POS tagging, NER, sentiment analysis, vectorization, topic modeling, and custom classifiers.  \n",
    "- ğŸ† Includes a **real-world case study** and wraps up with **deep learning & LLMs** in NLP.  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ Now that weâ€™ve set the stage, letâ€™s dive into our first lesson:  \n",
    "**What do we mean by Natural Language Processing?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19da935-16a2-4896-bb58-670080d57d52",
   "metadata": {},
   "source": [
    "# ğŸ§  Introduction to NLP\n",
    "\n",
    "## ğŸ“– What is Natural Language Processing (NLP)?\n",
    "Natural Language Processing (**NLP**) is a field of **artificial intelligence** that enables computers to **understand, interpret, and generate human language**, similar to how humans communicate with one another.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Techniques Used in NLP\n",
    "- ğŸ“Š **Statistics**\n",
    "- ğŸ¤– **Machine Learning**\n",
    "- ğŸ§® **Deep Learning**\n",
    "\n",
    "These approaches power modern NLP systems to analyze and process large volumes of text data efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›ï¸ Origins of NLP\n",
    "- ğŸ•°ï¸ **1950s** â†’ Early NLP methods focused on **rule-based systems**.  \n",
    "- ğŸ“œ These systems relied on **grammatical rules** to process text.  \n",
    "- âš ï¸ But just like in real life, grammar rules alone are **not enough** for true language understanding.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Modern Advancements\n",
    "- ğŸ’¾ Availability of **large datasets**  \n",
    "- âš¡ **Massive improvements in computing power**  \n",
    "- ğŸ¤– Breakthroughs in **deep learning architectures**  \n",
    "\n",
    "ğŸ‘‰ These advancements enabled the rise of **transformative systems like ChatGPT**, capable of human-like language understanding and generation.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Why NLP Matters for Data Scientists\n",
    "- ğŸ” Gain insights from large collections of text data  \n",
    "- â³ Save **significant time** by automating manual analysis  \n",
    "- ğŸ’¡ Discover insights that were previously **inaccessible or invisible**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "- ğŸ§  NLP enables computers to **understand, interpret, and generate human language data**.  \n",
    "- âš™ï¸ It leverages **statistics, machine learning, and deep learning**.  \n",
    "- ğŸ“œ Early **rule-based systems** were limited in understanding language.  \n",
    "- ğŸš€ **Modern NLP models** (like ChatGPT) are powerful tools for extracting insights from text at scale.  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‘‰ **Next Lesson:** Real-world examples of how NLP systems interact with our everyday lives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea32c58-e7e4-4ee8-be84-1bfe8990a6ba",
   "metadata": {},
   "source": [
    "# ğŸŒ NLP in Everyday Life\n",
    "\n",
    "Letâ€™s illustrate how far **Natural Language Processing (NLP)** has come and how much it interacts with our everyday lives through a few examples.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” NLP in Search Engines\n",
    "- Search engines use NLP to **understand a user's query** and provide relevant search results.  \n",
    "- NLP techniques help:  \n",
    "  - ğŸ“ Extract keywords  \n",
    "  - ğŸ¯ Comprehend the **intent** behind the query  \n",
    "  - ğŸŒ Return matching web pages  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“§ Spam Detection in Email\n",
    "- NLP powers **automatic spam filtering** in email systems.  \n",
    "- ğŸ§® **Classification algorithms** analyze patterns to:  \n",
    "  - ğŸš« Identify unwanted messages  \n",
    "  - âœ… Distinguish legitimate emails  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤– Customer Support & Chatbots\n",
    "- Chatbots and support systems rely on NLP to **understand customer queries**.  \n",
    "- Data scientists design **conversational agents** that can:  \n",
    "  - ğŸ’¬ Interpret customer intent  \n",
    "  - ğŸ“Œ Provide accurate, context-aware responses  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ Explore More\n",
    "There are many more examples of NLP applications around you â€” from **voice assistants** to **language translation tools**.  \n",
    "By the end of this course, youâ€™ll have the **foundational skills** to begin creating your own NLP solutions. ğŸš€  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "- ğŸŒ NLP is integral to everyday technologies like **search engines, spam filters, and chatbots**.  \n",
    "- ğŸ” Search engines leverage NLP to **extract keywords, understand intent, and serve relevant results**.  \n",
    "- ğŸ“§ **Classification algorithms** separate spam from legitimate emails.  \n",
    "- ğŸ¤– Conversational agents use NLP to **interpret queries and provide responses**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a94afe-6388-44d3-8837-2376a0b63051",
   "metadata": {},
   "source": [
    "# ğŸ§  Supervised vs Unsupervised NLP\n",
    "\n",
    "## ğŸ“– Introduction\n",
    "Supervised and unsupervised learning are two **fundamental approaches** in tackling NLP problems.  \n",
    "The method you choose depends on:  \n",
    "- ğŸ“Š The **data available**  \n",
    "- ğŸ¯ The **questions you want to answer**\n",
    "\n",
    "This course will cover **both approaches**, so letâ€™s break them down.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Supervised Learning\n",
    "Supervised learning = **learning with guidance (labels)**.  \n",
    "- You provide both:  \n",
    "  - ğŸ“ Input â†’ the **text data**  \n",
    "  - ğŸ¯ Output â†’ the **labels (scores, categories, etc.)**\n",
    "\n",
    "ğŸ“Œ **Example:**  \n",
    "Imagine you have a dataset of product reviews.  \n",
    "- Each review has:  \n",
    "  - ğŸ’¬ Review text  \n",
    "  - â­ A review score (e.g., 7/10)  \n",
    "- A supervised ML model can **learn the relationship** between text and score.  \n",
    "- Later, it can **predict the score** of a new, unseen review.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Unsupervised Learning\n",
    "Unsupervised learning = **no labels required**.  \n",
    "- The algorithm finds **patterns and structures** in the text by itself.  \n",
    "\n",
    "ğŸ“Œ **Example:**  \n",
    "- **Clustering** reviews into groups:  \n",
    "  - ğŸ“¦ Positive tone  \n",
    "  - ğŸ“¦ Neutral tone  \n",
    "  - ğŸ“¦ Negative tone  \n",
    "\n",
    "Even without knowing the â€œrightâ€ answer beforehand, the model can still uncover **hidden insights**.\n",
    "\n",
    "---\n",
    "\n",
    "## âš–ï¸ Choosing Between Them\n",
    "- âœ… **Use Supervised** when you have **labeled data** and want to **predict or classify**.  \n",
    "- âœ… **Use Unsupervised** when you have **unlabeled data** and want to **find patterns**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "- ğŸ“ **Supervised learning** = requires labeled data to map inputs â†’ outputs.  \n",
    "- ğŸ” **Unsupervised learning** = works without labels, finds natural patterns.  \n",
    "- âš–ï¸ The choice depends on **data availability** and the **insight you need**.  \n",
    "- ğŸ“¦ **Clustering** is a classic unsupervised method in NLP.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345861ca-8f59-47af-a4a7-1f3c7bc92078",
   "metadata": {},
   "source": [
    "# ğŸ“˜ The Importance of Data Preparation\n",
    "\n",
    "In **Natural Language Processing (NLP)**, the accuracy of any **machine learning model** or **insight** heavily depends on the **quality of the data** provided and how well that data has been **cleaned and prepared**.\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Garbage In, Garbage Out\n",
    "\n",
    "- The phrase **â€œgarbage in, garbage outâ€ (GIGO)** means:\n",
    "  - If we feed an algorithm **dirty, unstructured, noisy data**,  \n",
    "  - â€¦then the **results will also be poor**, regardless of the model used.  \n",
    "\n",
    "ğŸ‘‰ **Conclusion:** *Bad data = bad predictions*.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Steps in Text Data Preprocessing\n",
    "\n",
    "### 1. **General Cleaning**\n",
    "- Organize the dataset  \n",
    "- Tidy up text  \n",
    "- Remove problematic elements that may cause errors  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Noise Removal**\n",
    "- Eliminate unnecessary data that adds no value  \n",
    "- Examples:\n",
    "  - HTML tags  \n",
    "  - Special symbols  \n",
    "  - Extra white spaces  \n",
    "  - Stop words (like â€œisâ€, â€œtheâ€, â€œofâ€)  \n",
    "- Benefits:\n",
    "  - Saves **memory space**  \n",
    "  - Produces a **smaller, cleaner dataset**  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Formatting for ML Algorithms**\n",
    "- Ensure the data is in a **consistent format** suitable for ML models  \n",
    "- Examples:\n",
    "  - Lowercasing text  \n",
    "  - Tokenization  \n",
    "  - Lemmatization or stemming  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Transformation Flow\n",
    "\n",
    "Before preprocessing:\n",
    "```\n",
    "\"Hello!!!   This   is   an <b>Example</b>.....\"\n",
    "```\n",
    "\n",
    "After preprocessing:\n",
    "```\n",
    "\"hello example\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Diagram: Data Preparation Flow\n",
    "\n",
    "```plaintext\n",
    "Raw Text  â”€â”€â–º General Cleaning â”€â”€â–º Noise Removal â”€â”€â–º Formatting â”€â”€â–º ML-Ready Data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Python Example: Text Cleaning\n",
    "\n",
    "```python\n",
    "import re\n",
    "import string\n",
    "\n",
    "text = \"Hello!!!   This   is   an <b>Example</b>.....\"\n",
    "\n",
    "# Step 1: Remove HTML tags\n",
    "cleaned = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "# Step 2: Remove punctuation\n",
    "cleaned = cleaned.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Step 3: Normalize spaces & lowercase\n",
    "cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip().lower()\n",
    "\n",
    "print(cleaned)\n",
    "```\n",
    "\n",
    "```output\n",
    "hello this is an example\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "\n",
    "- **Quality and cleanliness of data** are crucial for accuracy in NLP.  \n",
    "- **Garbage in, garbage out (GIGO):** Poor data quality = poor performance.  \n",
    "- **Preprocessing steps:**\n",
    "  1. General Cleaning  \n",
    "  2. Noise Removal  \n",
    "  3. Formatting for algorithms  \n",
    "- **Outcome:** Transforms raw, messy text into a **structured, ML-ready format**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0785d58-bcf5-4034-ab42-f5d6005712d4",
   "metadata": {},
   "source": [
    "# ğŸ”¡ Lowercasing Text in NLP\n",
    "\n",
    "## ğŸ“– Why Lowercase?\n",
    "An important first step in working with text data is **converting it into lowercase**.  \n",
    "\n",
    "### âœ¨ Benefits:\n",
    "- ğŸ”„ Maintains **consistency** in data  \n",
    "- ğŸ§® Ensures words are **counted the same** (`Apple` = `apple`)  \n",
    "- ğŸ¤– Helps ML models treat words uniformly  \n",
    "- ğŸ§¹ Makes **further cleaning easier** (no need to handle cases separately)  \n",
    "\n",
    "âš ï¸ **Caution:**  \n",
    "Lowercasing can sometimes **change meaning**:  \n",
    "- `\"US\"` ğŸ‡ºğŸ‡¸ â†’ a country  \n",
    "- `\"us\"` ğŸ™‹ â†’ pronoun  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Python Example\n",
    "\n",
    "### âœ… Lowercasing a Single Sentence\n",
    "```python\n",
    "sentence = \"Her cat's name is Luna.\"\n",
    "lower_sentence = sentence.lower()\n",
    "print(lower_sentence)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "her cat's name is luna.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Lowercasing a List of Sentences\n",
    "```python\n",
    "sentence_list = [\"Her cat's name is Luna.\", \"This is a Test.\", \"Python is Fun!\"]\n",
    "lower_sentence_list = [x.lower() for x in sentence_list]\n",
    "print(lower_sentence_list)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "[\"her cat's name is luna.\", \"this is a test.\", \"python is fun!\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Key Takeaways\n",
    "- ğŸ”¡ **Lowercasing ensures consistency** in text data.  \n",
    "- ğŸ¤– Words with different cases (`Dog` vs `dog`) are treated **as the same**.  \n",
    "- ğŸ§¹ Simplifies further **data cleaning steps**.  \n",
    "- âš ï¸ Be careful with acronyms & proper nouns (`US` â‰  `us`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa066184-31ec-4a8f-a65c-4b74afc25872",
   "metadata": {},
   "source": [
    "# ğŸ“ Removing Stopwords with NLTK\n",
    "\n",
    "## ğŸš€ Introduction to Removing Stopwords\n",
    "In this lesson, we will use the **NLTK** package to remove stopwords from our text.\n",
    "\n",
    "ğŸ‘‰ Stopwords are common words in the language that do not carry much meaning.  \n",
    "Examples: **\"and,\" \"of,\" \"a,\" \"to\"**  \n",
    "\n",
    "Removing stopwords helps by:\n",
    "- âš¡ Reducing complexity in the dataset  \n",
    "- ğŸ¯ Improving machine learning accuracy  \n",
    "- â© Speeding up processing time  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¥ Importing NLTK and Downloading Stopwords\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "If you have not already downloaded these, it may take a few minutes.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Assigning and Printing Stopwords\n",
    "\n",
    "```python\n",
    "n_stopwords = stopwords.words('english')\n",
    "print(n_stopwords)\n",
    "```\n",
    "\n",
    "âœ… This gives you a list of common English stopwords.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ Removing Stopwords from a Sentence\n",
    "\n",
    "Sentence to process:\n",
    "```python\n",
    "sentence = 'It was too far to go to the shop and he did not want her to walk.'\n",
    "```\n",
    "\n",
    "Remove stopwords:\n",
    "```python\n",
    "sentence_no_stopwords = ' '.join([word for word in sentence.split() if word.lower() not in n_stopwords])\n",
    "print(sentence_no_stopwords)\n",
    "```\n",
    "\n",
    "ğŸ“Œ Output:\n",
    "```\n",
    "far go shop. Want walk.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Customizing the Stopwords List\n",
    "\n",
    "You can modify the list:  \n",
    "\n",
    "```python\n",
    "n_stopwords.remove('did')\n",
    "n_stopwords.remove('not')\n",
    "n_stopwords.append('go')\n",
    "\n",
    "sentence_no_stopwords_custom = ' '.join([word for word in sentence.split() if word.lower() not in n_stopwords])\n",
    "print(sentence_no_stopwords_custom)\n",
    "```\n",
    "\n",
    "ğŸ“Œ Output:\n",
    "```\n",
    "far shop, did not want walk\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "\n",
    "- ğŸ› ï¸ **NLTK** provides built-in stopwords lists for many languages.  \n",
    "- ğŸ§¹ Removing stopwords simplifies text data and speeds up processing.  \n",
    "- ğŸ¯ Cleaner datasets often improve ML model performance.  \n",
    "- âœï¸ You can customize stopword lists by adding or removing words.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7df23-e1b7-483d-b851-35b56d612e24",
   "metadata": {},
   "source": [
    "# Regular Expressions in NLP\n",
    "\n",
    "## ğŸ“– Introduction to Regular Expressions\n",
    "Regular expressions, or **regex**, are a special syntax for searching strings that match specified patterns.  \n",
    "They are a powerful tool to filter and sort through text when you want to **match patterns** instead of exact strings.  \n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Importing the `re` Package\n",
    "```python\n",
    "import re\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Raw Strings in Python\n",
    "Python treats characters like `\\n` as special (newline).  \n",
    "To avoid misinterpretation, prefix strings with `r` to indicate a **raw string**.\n",
    "\n",
    "```python\n",
    "my_folder = \"c\\desktop\\notes\"   # Interprets \n",
    " as newline âŒ\n",
    "print(my_folder)\n",
    "\n",
    "my_folder_raw = r\"c\\desktop\\notes\"  # Treated literally âœ…\n",
    "print(my_folder_raw)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Regex Functions Overview\n",
    "\n",
    "- **`re.search(pattern, string)`** â†’ returns match if found, else `None`  \n",
    "- **`re.sub(pattern, replacement, string)`** â†’ replaces matched text  \n",
    "\n",
    "```python\n",
    "text = \"Sara was able to help me quickly.\"\n",
    "new_text = re.sub(\"Sara\", \"Sarah\", text)\n",
    "print(new_text)  # Sarah was able to help me quickly.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Regex Syntax Examples\n",
    "\n",
    "### âœ… Matching Optional Characters with `?`\n",
    "```python\n",
    "reviews = [\"Sara helped a lot.\", \"Sarah was kind.\"]\n",
    "pattern = r\"Sara?h\"\n",
    "for review in reviews:\n",
    "    if re.search(pattern, review):\n",
    "        print(review)\n",
    "# Matches both Sara and Sarah\n",
    "```\n",
    "\n",
    "### âœ… Start of String `^`\n",
    "```python\n",
    "re.search(r\"^A\", \"Amazing work!\")  # Matches\n",
    "```\n",
    "\n",
    "### âœ… End of String `$`\n",
    "```python\n",
    "re.search(r\"y$\", \"Great delivery\")  # Matches 'y' at end\n",
    "```\n",
    "\n",
    "### âœ… Alternation `|`\n",
    "```python\n",
    "pattern = r\"(need|want)ed\"\n",
    "text = \"I needed help and she wanted answers.\"\n",
    "print(re.findall(pattern, text))  # ['need', 'want']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ Removing Punctuation\n",
    "```python\n",
    "reviews = [\"Amazing work!\", \"Really helpful :)\"]\n",
    "cleaned = [re.sub(r\"[^\\w\\s]\", \"\", review) for review in reviews]\n",
    "print(cleaned)  # ['Amazing work', 'Really helpful ']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¨ Regex Cheat Sheet (Visual)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A(^ = Start of String) --> B($ = End of String)\n",
    "    B --> C(? = Optional Character)\n",
    "    C --> D(| = OR / Alternation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Key Takeaways\n",
    "- Regex provides **pattern-based searching** instead of raw string matching.  \n",
    "- Use `r\"\"` raw strings to avoid escape issues.  \n",
    "- Common functions: `re.search`, `re.sub`.  \n",
    "- Syntax:  \n",
    "  - `?` â†’ optional character  \n",
    "  - `^` â†’ start of string  \n",
    "  - `$` â†’ end of string  \n",
    "  - `|` â†’ alternation  \n",
    "- Regex is essential for **text preprocessing** (e.g., punctuation removal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80b393-e99b-4478-b5ab-6dfd9f183841",
   "metadata": {},
   "source": [
    "# ğŸ“ Tokenization\n",
    "\n",
    "## ğŸ”¹ Introduction\n",
    "A fundamental step in **Natural Language Processing (NLP)** involves converting our text into smaller units through a process known as **tokenization**.  \n",
    "These smaller units are called **tokens**.\n",
    "\n",
    "- **Word Tokenization** â†’ Breaks text into words.  \n",
    "- **Sentence Tokenization** â†’ Breaks text into sentences.  \n",
    "- Tokens can also be **subwords** or **characters**, depending on the use case.\n",
    "\n",
    "ğŸ‘‰ We perform tokenization because the meaning of text is better understood if we can analyze the **individual parts** as well as the **whole**.  \n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ Sentence Tokenization Example\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"Her cat's name is Luna. Her dog's name is Max.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "```\n",
    "\n",
    "### âœ… Expected Output\n",
    "```python\n",
    "[\"Her cat's name is Luna.\", \"Her dog's name is Max.\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¤ Word Tokenization Example\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word tokenization on a single sentence\n",
    "sentence = \"Her cat's name is Luna.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "```\n",
    "\n",
    "### âœ… Expected Output\n",
    "```python\n",
    "['Her', 'cat', \"'s\", 'name', 'is', 'Luna', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Combined Tokenization Example\n",
    "```python\n",
    "# Word tokenization on the full text\n",
    "words_full = word_tokenize(text)\n",
    "print(words_full)\n",
    "\n",
    "# Convert all tokens to lowercase for consistency\n",
    "words_lower = [w.lower() for w in words_full]\n",
    "print(words_lower)\n",
    "```\n",
    "\n",
    "### âœ… Expected Output\n",
    "```python\n",
    "['Her', 'cat', \"'s\", 'name', 'is', 'Luna', '.', 'Her', 'dog', \"'s\", 'name', 'is', 'Max', '.']\n",
    "\n",
    "['her', 'cat', \"'s\", 'name', 'is', 'luna', '.', 'her', 'dog', \"'s\", 'name', 'is', 'max', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Key Takeaways\n",
    "- Tokenization = splitting text into smaller units (**tokens**).  \n",
    "- Sentence tokenization â†’ breaks text into **sentences**.  \n",
    "- Word tokenization â†’ breaks text into **words**.  \n",
    "- Lowercasing tokens ensures consistency when analyzing frequencies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c319d-cc23-4863-8089-3200c4ed60ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ğŸ”„ Stemming in NLP\n",
    "\n",
    "Stemming is the process of reducing words to their **base or root form**.  \n",
    "It is part of **text standardization** during preprocessing.\n",
    "\n",
    "For example:  \n",
    "- **connecting â†’ connect**  \n",
    "- **connected â†’ connect**  \n",
    "- **connectivity â†’ connect**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒŸ Why Use Stemming?\n",
    "- Reduces the **number of unique words** in the dataset.  \n",
    "- Simplifies and lowers the **complexity** of data.  \n",
    "- Makes models focus on the **core meaning** instead of variations.  \n",
    "\n",
    "> [!IMPORTANT]  \n",
    "> Sometimes stemming produces non-words (e.g., *worse â†’ wos*).  \n",
    "> Always balance simplicity vs. semantic accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Using NLTKâ€™s Porter Stemmer\n",
    "\n",
    "### Example 1: Words around **connect**\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "PPS = PorterStemmer()\n",
    "\n",
    "tokens_connect = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "for token in tokens_connect:\n",
    "    print(token, \"â†’\", PPS.stem(token))\n",
    "```\n",
    "\n",
    "âœ… Output:\n",
    "```\n",
    "connecting â†’ connect\n",
    "connected â†’ connect\n",
    "connectivity â†’ connect\n",
    "connect â†’ connect\n",
    "connects â†’ connect\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Words around **learn**\n",
    "```python\n",
    "tokens_learn = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "for token in tokens_learn:\n",
    "    print(token, \"â†’\", PPS.stem(token))\n",
    "```\n",
    "\n",
    "âœ… Output:\n",
    "```\n",
    "learned â†’ learn\n",
    "learning â†’ learn\n",
    "learn â†’ learn\n",
    "learns â†’ learn\n",
    "learner â†’ learner\n",
    "learners â†’ learner\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Irregular cases\n",
    "```python\n",
    "tokens_misc = ['likes', 'better', 'worse']\n",
    "for token in tokens_misc:\n",
    "    print(token, \"â†’\", PPS.stem(token))\n",
    "```\n",
    "\n",
    "âœ… Output:\n",
    "```\n",
    "likes â†’ like\n",
    "better â†’ better\n",
    "worse â†’ wors\n",
    "```\n",
    "\n",
    "> [!NOTE]  \n",
    "> Notice how *worse* is stemmed to *wors*. This shows that stemming is **rule-based**, not meaning-based.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Stemming Process Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Original Words] --> B[Stemming Rules]\n",
    "    B --> C[Base Form Tokens]\n",
    "    \n",
    "    A:::aStyle\n",
    "    B:::bStyle\n",
    "    C:::cStyle\n",
    "\n",
    "    classDef aStyle fill:#FFDDC1,stroke:#333,stroke-width:2px;\n",
    "    classDef bStyle fill:#C1E1FF,stroke:#333,stroke-width:2px;\n",
    "    classDef cStyle fill:#C1FFD7,stroke:#333,stroke-width:2px;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "- ğŸ”¡ **Stemming** reduces words to their base/root form by chopping suffixes.  \n",
    "- ğŸ“‰ Reduces dataset size and complexity.  \n",
    "- âš¡ **Porter Stemmer** is widely used in Python via NLTK.  \n",
    "- âš ï¸ Not always linguistically correct (produces non-words).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bb3c5-6ce5-4eef-bd5c-b0cbfc4f9f4e",
   "metadata": {},
   "source": [
    "# ğŸ“ Lemmatization in NLP\n",
    "\n",
    "## ğŸ“– Introduction\n",
    "Where **stemming** removes the last few characters of a word, **lemmatization** reduces words to a more meaningful base form and ensures they do not lose their meaning.\n",
    "\n",
    "ğŸ”‘ **Key Difference:**  \n",
    "- **Stemming**: Cuts off word endings (may result in non-meaningful roots).  \n",
    "- **Lemmatization**: Uses a predefined **dictionary (WordNet)** and **context** to ensure the base form is meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ How Lemmatization Works\n",
    "- Lemmatization works more intelligently by referencing a **predefined dictionary** containing the context of the word.  \n",
    "- Words are reduced only if their meaningful lemma exists.  \n",
    "- âœ… More accurate than stemming, but results in a larger dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Python Example â€“ Using NLTK\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example tokens\n",
    "tokens_connect = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "tokens_learn = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "tokens_likes = ['likes', 'better', 'worse']\n",
    "\n",
    "print(\"ğŸ“Œ Connect tokens:\")\n",
    "for token in tokens_connect:\n",
    "    print(f\"{token} â {lemmatizer.lemmatize(token)}\")\n",
    "\n",
    "print(\"\\nğŸ“Œ Learn tokens:\")\n",
    "for token in tokens_learn:\n",
    "    print(f\"{token} â {lemmatizer.lemmatize(token)}\")\n",
    "\n",
    "print(\"\\nğŸ“Œ Likes tokens:\")\n",
    "for token in tokens_likes:\n",
    "    print(f\"{token} â {lemmatizer.lemmatize(token)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Example Output\n",
    "\n",
    "```\n",
    "ğŸ“Œ Connect tokens:\n",
    "connecting â connecting\n",
    "connected â connected\n",
    "connectivity â connectivity\n",
    "connect â connect\n",
    "connects â connects\n",
    "\n",
    "ğŸ“Œ Learn tokens:\n",
    "learned â learned\n",
    "learning â learning\n",
    "learn â learn\n",
    "learns â learns\n",
    "learner â learner\n",
    "learners â learner\n",
    "\n",
    "ğŸ“Œ Likes tokens:\n",
    "likes â like\n",
    "better â better\n",
    "worse â worse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Stemming vs Lemmatization\n",
    "\n",
    "| Feature               | Stemming | Lemmatization |\n",
    "|-----------------------|----------|---------------|\n",
    "| Uses dictionary?      | âŒ No    | âœ… Yes |\n",
    "| Speed                 | âš¡ Faster | ğŸ¢ Slower |\n",
    "| Accuracy              | âŒ May produce non-words (e.g., *worse â†’ wor*) | âœ… Produces meaningful words (e.g., *worse â†’ worse*) |\n",
    "| Data size reduction   | âœ… Strong | âŒ Weak |\n",
    "| Preserves meaning     | âŒ Not always | âœ… Yes |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Simple Diagram\n",
    "\n",
    "```\n",
    "   Words â†’ [Stemming] â†’ \"wors\"\n",
    "        â†˜ [Lemmatization] â†’ \"worse\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Key Takeaways\n",
    "- âœ‚ï¸ **Stemming** removes suffixes, sometimes breaking word meaning.  \n",
    "- ğŸ“š **Lemmatization** reduces words to valid base forms using a dictionary.  \n",
    "- ğŸ“Š Lemmatization preserves **semantic meaning** but increases dataset size.  \n",
    "- âš¡ Use **stemming** when speed matters, **lemmatization** when accuracy matters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e36fa6-fa49-49a8-b91f-d2e52dc351e9",
   "metadata": {},
   "source": [
    "# ğŸ“š N-grams in NLP\n",
    "N-grams help us **inspect preprocessing**, **explore data**, and **engineer features** for ML.  \n",
    "An **n-gram** is a sequence of **n neighboring tokens** (words, subwords, or chars).\n",
    "\n",
    "> [!TIP]\n",
    "> Use n-grams after basic cleaning (lowercase, tokenization, stopword handling) to get clearer signal.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Requirements\n",
    "```python\n",
    "# Core packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "# (no output on import)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Example Tokens (preprocessed-lite)\n",
    "These are sample tokens (lowercased, light cleaning) for demonstration:\n",
    "\n",
    "```python\n",
    "tokens = [\n",
    "    'two','of','the','cats','that','were','here',\n",
    "    'two','of','the','dogs','that','were','barking',\n",
    "    'two','of','the','birds','that','were','singing',\n",
    "    'two','of','them','were','happy'\n",
    "]\n",
    "print(tokens[:12])\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "['two', 'of', 'the', 'cats', 'that', 'were', 'here', 'two', 'of', 'the', 'dogs', 'that']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Unigrams (n = 1)\n",
    "\n",
    "```python\n",
    "# Count unigrams\n",
    "uni_counts = Counter(tokens)\n",
    "# Or via pandas for convenience\n",
    "uni_series = pd.Series(tokens).value_counts()\n",
    "\n",
    "print(\"Top unigrams (Counter):\", uni_counts.most_common(5))\n",
    "print(\"\\nTop unigrams (pandas):\\n\", uni_series.head(10))\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "Top unigrams (Counter): [('two', 4), ('of', 4), ('were', 4), ('the', 3), ('that', 3)]\n",
    "\n",
    "Top unigrams (pandas):\n",
    "two      4\n",
    "of       4\n",
    "were     4\n",
    "the      3\n",
    "that     3\n",
    "cats     1\n",
    "here     1\n",
    "dogs     1\n",
    "barking  1\n",
    "birds    1\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "### ğŸ“Š Visualize Top-10 Unigrams\n",
    "> [!NOTE]\n",
    "> Plotting uses **matplotlib** (single chart, default colors).\n",
    "\n",
    "```python\n",
    "top10 = uni_series.head(10).sort_values()\n",
    "plt.figure(figsize=(8, 4))\n",
    "top10.plot.barh()\n",
    "plt.title(\"Top 10 Unigrams\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "# A horizontal bar chart is displayed.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Bigrams (n = 2)\n",
    "```python\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "bigram_counts = Counter(bigrams)\n",
    "print(bigram_counts.most_common(5))\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "[(('two', 'of'), 4), (('of', 'the'), 3), (('that', 'were'), 3), (('the', 'cats'), 1), (('cats', 'that'), 1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Trigrams (n = 3)\n",
    "```python\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "trigram_counts = Counter(trigrams)\n",
    "print(trigram_counts.most_common(5))\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "[(('two', 'of', 'the'), 3), (('of', 'the', 'cats'), 1), (('the', 'cats', 'that'), 1), (('cats', 'that', 'were'), 1), (('that', 'were', 'here'), 1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Quick Helper: N-gram Frequencies\n",
    "```python\n",
    "def ngram_freq(tokens, n=2, top_k=10):\n",
    "    counts = Counter(ngrams(tokens, n))\n",
    "    return counts.most_common(top_k)\n",
    "\n",
    "print(\"Top-10 bigrams:\", ngram_freq(tokens, n=2, top_k=10))\n",
    "print(\"Top-10 trigrams:\", ngram_freq(tokens, n=3, top_k=10))\n",
    "```\n",
    "âœ… Output:\n",
    "```\n",
    "Top-10 bigrams: [(('two', 'of'), 4), (('of', 'the'), 3), (('that', 'were'), 3), (('the', 'cats'), 1), (('cats', 'that'), 1), (('the', 'dogs'), 1), (('dogs', 'that'), 1), (('were', 'barking'), 1), (('the', 'birds'), 1), (('birds', 'that'), 1)]\n",
    "Top-10 trigrams: [(('two', 'of', 'the'), 3), (('of', 'the', 'cats'), 1), (('the', 'cats', 'that'), 1), (('cats', 'that', 'were'), 1), (('that', 'were', 'here'), 1), (('of', 'the', 'dogs'), 1), (('the', 'dogs', 'that'), 1), (('dogs', 'that', 'were'), 1), (('that', 'were', 'barking'), 1), (('of', 'the', 'birds'), 1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§­ How N-grams Slide (Diagram)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A1[\"Tokens: two â†’ of â†’ the â†’ cats â†’ that â†’ were â†’ ...\"]\n",
    "    A2[\"Window (n=2): [two of] â†’ [of the] â†’ [the cats] â†’ ...\"]\n",
    "    A3[\"Window (n=3): [two of the] â†’ [of the cats] â†’ [the cats that] â†’ ...\"]\n",
    "    A1 --> A2 --> A3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "- An **n-gram** is a sequence of **n neighboring tokens**.  \n",
    "- **Unigrams, bigrams, trigrams** correspond to **n = 1, 2, 3**.  \n",
    "- With **NLTK + pandas**, you can **count** and **visualize** n-grams quickly.  \n",
    "- N-gram analysis becomes powerful after **thorough preprocessing** and on **larger corpora**.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> For production features, consider:\n",
    "> - filtering stopwords\n",
    "> - normalizing (lowercase, stemming/lemmatization)\n",
    "> - min-frequency thresholds to reduce noise\n",
    "> - character n-grams for languages without whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b48b5-1e33-4a67-8b02-d8c135e07361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
