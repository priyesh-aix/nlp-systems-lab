{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7907e601-6ac1-40a3-97ef-140e437d087b",
   "metadata": {},
   "source": [
    "# 🚀 Introduction to the Course\n",
    "\n",
    "Hello and welcome to this **Introduction to Natural Language Processing (NLP)** course.  \n",
    "We will be delving into the exciting field of NLP and exploring techniques that enable computers to **understand, generate, and classify human language**.\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 What You Need\n",
    "- ✅ Basic Python skills  \n",
    "- ✅ Some familiarity with machine learning  \n",
    "- ❌ No prior NLP knowledge required  \n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Course Roadmap\n",
    "\n",
    "1. **Introduction to NLP**  \n",
    "   - What NLP means  \n",
    "   - Everyday applications of NLP  \n",
    "\n",
    "2. **🔧 Text Pre-processing**  \n",
    "   - Fundamental step in NLP  \n",
    "   - Practical exercises included  \n",
    "\n",
    "3. **🧩 Key Component Extraction**  \n",
    "   - Parts of Speech (POS) Tagging  \n",
    "   - Named Entity Recognition (NER)  \n",
    "\n",
    "4. **💬 Sentiment Analysis**  \n",
    "   - Understanding emotions in text  \n",
    "\n",
    "5. **🔢 Text Vectorization**  \n",
    "   - Preparing text data for machine learning  \n",
    "\n",
    "6. **📊 Advanced Topics**  \n",
    "   - Topic modeling  \n",
    "   - Building a custom text classifier  \n",
    "\n",
    "7. **🏢 Case Study (Section A)**  \n",
    "   - End-to-end business problem solution  \n",
    "   - Portfolio-ready Jupyter Notebook  \n",
    "\n",
    "8. **🔮 Future of NLP**  \n",
    "   - Deep Learning in NLP  \n",
    "   - Large Language Models (LLMs)  \n",
    "   - Trends & future directions  \n",
    "\n",
    "---\n",
    "\n",
    "## 👩‍🏫 Instructor Introduction\n",
    "\n",
    "**Lauren Newbold**  \n",
    "- Data Scientist with experience in **large organizations and startups**  \n",
    "- Built a **custom NLP text classifier** for interview data in developing communities  \n",
    "- Speaker at **international conferences** on NLP in developing countries  \n",
    "- Passionate about **teaching and making data exciting**  \n",
    "\n",
    "---\n",
    "\n",
    "## ✨ Key Takeaways\n",
    "- 🤖 NLP enables computers to **understand, generate, and classify** human language.  \n",
    "- 🐍 Requires **basic Python skills** and some ML knowledge — no prior NLP experience.  \n",
    "- 📘 Covers: preprocessing, POS tagging, NER, sentiment analysis, vectorization, topic modeling, and custom classifiers.  \n",
    "- 🏆 Includes a **real-world case study** and wraps up with **deep learning & LLMs** in NLP.  \n",
    "\n",
    "---\n",
    "\n",
    "👉 Now that we’ve set the stage, let’s dive into our first lesson:  \n",
    "**What do we mean by Natural Language Processing?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19da935-16a2-4896-bb58-670080d57d52",
   "metadata": {},
   "source": [
    "# 🧠 Introduction to NLP\n",
    "\n",
    "## 📖 What is Natural Language Processing (NLP)?\n",
    "Natural Language Processing (**NLP**) is a field of **artificial intelligence** that enables computers to **understand, interpret, and generate human language**, similar to how humans communicate with one another.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Techniques Used in NLP\n",
    "- 📊 **Statistics**\n",
    "- 🤖 **Machine Learning**\n",
    "- 🧮 **Deep Learning**\n",
    "\n",
    "These approaches power modern NLP systems to analyze and process large volumes of text data efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏛️ Origins of NLP\n",
    "- 🕰️ **1950s** → Early NLP methods focused on **rule-based systems**.  \n",
    "- 📜 These systems relied on **grammatical rules** to process text.  \n",
    "- ⚠️ But just like in real life, grammar rules alone are **not enough** for true language understanding.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Modern Advancements\n",
    "- 💾 Availability of **large datasets**  \n",
    "- ⚡ **Massive improvements in computing power**  \n",
    "- 🤖 Breakthroughs in **deep learning architectures**  \n",
    "\n",
    "👉 These advancements enabled the rise of **transformative systems like ChatGPT**, capable of human-like language understanding and generation.\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Why NLP Matters for Data Scientists\n",
    "- 🔍 Gain insights from large collections of text data  \n",
    "- ⏳ Save **significant time** by automating manual analysis  \n",
    "- 💡 Discover insights that were previously **inaccessible or invisible**  \n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Key Takeaways\n",
    "- 🧠 NLP enables computers to **understand, interpret, and generate human language data**.  \n",
    "- ⚙️ It leverages **statistics, machine learning, and deep learning**.  \n",
    "- 📜 Early **rule-based systems** were limited in understanding language.  \n",
    "- 🚀 **Modern NLP models** (like ChatGPT) are powerful tools for extracting insights from text at scale.  \n",
    "\n",
    "---\n",
    "\n",
    "👉 **Next Lesson:** Real-world examples of how NLP systems interact with our everyday lives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea32c58-e7e4-4ee8-be84-1bfe8990a6ba",
   "metadata": {},
   "source": [
    "# 🌍 NLP in Everyday Life\n",
    "\n",
    "Let’s illustrate how far **Natural Language Processing (NLP)** has come and how much it interacts with our everyday lives through a few examples.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 NLP in Search Engines\n",
    "- Search engines use NLP to **understand a user's query** and provide relevant search results.  \n",
    "- NLP techniques help:  \n",
    "  - 📝 Extract keywords  \n",
    "  - 🎯 Comprehend the **intent** behind the query  \n",
    "  - 🌐 Return matching web pages  \n",
    "\n",
    "---\n",
    "\n",
    "## 📧 Spam Detection in Email\n",
    "- NLP powers **automatic spam filtering** in email systems.  \n",
    "- 🧮 **Classification algorithms** analyze patterns to:  \n",
    "  - 🚫 Identify unwanted messages  \n",
    "  - ✅ Distinguish legitimate emails  \n",
    "\n",
    "---\n",
    "\n",
    "## 🤖 Customer Support & Chatbots\n",
    "- Chatbots and support systems rely on NLP to **understand customer queries**.  \n",
    "- Data scientists design **conversational agents** that can:  \n",
    "  - 💬 Interpret customer intent  \n",
    "  - 📌 Provide accurate, context-aware responses  \n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Explore More\n",
    "There are many more examples of NLP applications around you — from **voice assistants** to **language translation tools**.  \n",
    "By the end of this course, you’ll have the **foundational skills** to begin creating your own NLP solutions. 🚀  \n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Key Takeaways\n",
    "- 🌍 NLP is integral to everyday technologies like **search engines, spam filters, and chatbots**.  \n",
    "- 🔎 Search engines leverage NLP to **extract keywords, understand intent, and serve relevant results**.  \n",
    "- 📧 **Classification algorithms** separate spam from legitimate emails.  \n",
    "- 🤖 Conversational agents use NLP to **interpret queries and provide responses**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a94afe-6388-44d3-8837-2376a0b63051",
   "metadata": {},
   "source": [
    "# 🧠 Supervised vs Unsupervised NLP\n",
    "\n",
    "## 📖 Introduction\n",
    "Supervised and unsupervised learning are two **fundamental approaches** in tackling NLP problems.  \n",
    "The method you choose depends on:  \n",
    "- 📊 The **data available**  \n",
    "- 🎯 The **questions you want to answer**\n",
    "\n",
    "This course will cover **both approaches**, so let’s break them down.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎓 Supervised Learning\n",
    "Supervised learning = **learning with guidance (labels)**.  \n",
    "- You provide both:  \n",
    "  - 📝 Input → the **text data**  \n",
    "  - 🎯 Output → the **labels (scores, categories, etc.)**\n",
    "\n",
    "📌 **Example:**  \n",
    "Imagine you have a dataset of product reviews.  \n",
    "- Each review has:  \n",
    "  - 💬 Review text  \n",
    "  - ⭐ A review score (e.g., 7/10)  \n",
    "- A supervised ML model can **learn the relationship** between text and score.  \n",
    "- Later, it can **predict the score** of a new, unseen review.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Unsupervised Learning\n",
    "Unsupervised learning = **no labels required**.  \n",
    "- The algorithm finds **patterns and structures** in the text by itself.  \n",
    "\n",
    "📌 **Example:**  \n",
    "- **Clustering** reviews into groups:  \n",
    "  - 📦 Positive tone  \n",
    "  - 📦 Neutral tone  \n",
    "  - 📦 Negative tone  \n",
    "\n",
    "Even without knowing the “right” answer beforehand, the model can still uncover **hidden insights**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Choosing Between Them\n",
    "- ✅ **Use Supervised** when you have **labeled data** and want to **predict or classify**.  \n",
    "- ✅ **Use Unsupervised** when you have **unlabeled data** and want to **find patterns**.  \n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Key Takeaways\n",
    "- 🎓 **Supervised learning** = requires labeled data to map inputs → outputs.  \n",
    "- 🔍 **Unsupervised learning** = works without labels, finds natural patterns.  \n",
    "- ⚖️ The choice depends on **data availability** and the **insight you need**.  \n",
    "- 📦 **Clustering** is a classic unsupervised method in NLP.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345861ca-8f59-47af-a4a7-1f3c7bc92078",
   "metadata": {},
   "source": [
    "# 📘 The Importance of Data Preparation\n",
    "\n",
    "In **Natural Language Processing (NLP)**, the accuracy of any **machine learning model** or **insight** heavily depends on the **quality of the data** provided and how well that data has been **cleaned and prepared**.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Garbage In, Garbage Out\n",
    "\n",
    "- The phrase **“garbage in, garbage out” (GIGO)** means:\n",
    "  - If we feed an algorithm **dirty, unstructured, noisy data**,  \n",
    "  - …then the **results will also be poor**, regardless of the model used.  \n",
    "\n",
    "👉 **Conclusion:** *Bad data = bad predictions*.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Steps in Text Data Preprocessing\n",
    "\n",
    "### 1. **General Cleaning**\n",
    "- Organize the dataset  \n",
    "- Tidy up text  \n",
    "- Remove problematic elements that may cause errors  \n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Noise Removal**\n",
    "- Eliminate unnecessary data that adds no value  \n",
    "- Examples:\n",
    "  - HTML tags  \n",
    "  - Special symbols  \n",
    "  - Extra white spaces  \n",
    "  - Stop words (like “is”, “the”, “of”)  \n",
    "- Benefits:\n",
    "  - Saves **memory space**  \n",
    "  - Produces a **smaller, cleaner dataset**  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Formatting for ML Algorithms**\n",
    "- Ensure the data is in a **consistent format** suitable for ML models  \n",
    "- Examples:\n",
    "  - Lowercasing text  \n",
    "  - Tokenization  \n",
    "  - Lemmatization or stemming  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Transformation Flow\n",
    "\n",
    "Before preprocessing:\n",
    "```\n",
    "\"Hello!!!   This   is   an <b>Example</b>.....\"\n",
    "```\n",
    "\n",
    "After preprocessing:\n",
    "```\n",
    "\"hello example\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Diagram: Data Preparation Flow\n",
    "\n",
    "```plaintext\n",
    "Raw Text  ──► General Cleaning ──► Noise Removal ──► Formatting ──► ML-Ready Data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 Python Example: Text Cleaning\n",
    "\n",
    "```python\n",
    "import re\n",
    "import string\n",
    "\n",
    "text = \"Hello!!!   This   is   an <b>Example</b>.....\"\n",
    "\n",
    "# Step 1: Remove HTML tags\n",
    "cleaned = re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "# Step 2: Remove punctuation\n",
    "cleaned = cleaned.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Step 3: Normalize spaces & lowercase\n",
    "cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip().lower()\n",
    "\n",
    "print(cleaned)\n",
    "```\n",
    "\n",
    "```output\n",
    "hello this is an example\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Takeaways\n",
    "\n",
    "- **Quality and cleanliness of data** are crucial for accuracy in NLP.  \n",
    "- **Garbage in, garbage out (GIGO):** Poor data quality = poor performance.  \n",
    "- **Preprocessing steps:**\n",
    "  1. General Cleaning  \n",
    "  2. Noise Removal  \n",
    "  3. Formatting for algorithms  \n",
    "- **Outcome:** Transforms raw, messy text into a **structured, ML-ready format**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0785d58-bcf5-4034-ab42-f5d6005712d4",
   "metadata": {},
   "source": [
    "# 🔡 Lowercasing Text in NLP\n",
    "\n",
    "## 📖 Why Lowercase?\n",
    "An important first step in working with text data is **converting it into lowercase**.  \n",
    "\n",
    "### ✨ Benefits:\n",
    "- 🔄 Maintains **consistency** in data  \n",
    "- 🧮 Ensures words are **counted the same** (`Apple` = `apple`)  \n",
    "- 🤖 Helps ML models treat words uniformly  \n",
    "- 🧹 Makes **further cleaning easier** (no need to handle cases separately)  \n",
    "\n",
    "⚠️ **Caution:**  \n",
    "Lowercasing can sometimes **change meaning**:  \n",
    "- `\"US\"` 🇺🇸 → a country  \n",
    "- `\"us\"` 🙋 → pronoun  \n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 Python Example\n",
    "\n",
    "### ✅ Lowercasing a Single Sentence\n",
    "```python\n",
    "sentence = \"Her cat's name is Luna.\"\n",
    "lower_sentence = sentence.lower()\n",
    "print(lower_sentence)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "her cat's name is luna.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Lowercasing a List of Sentences\n",
    "```python\n",
    "sentence_list = [\"Her cat's name is Luna.\", \"This is a Test.\", \"Python is Fun!\"]\n",
    "lower_sentence_list = [x.lower() for x in sentence_list]\n",
    "print(lower_sentence_list)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "```\n",
    "[\"her cat's name is luna.\", \"this is a test.\", \"python is fun!\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Key Takeaways\n",
    "- 🔡 **Lowercasing ensures consistency** in text data.  \n",
    "- 🤖 Words with different cases (`Dog` vs `dog`) are treated **as the same**.  \n",
    "- 🧹 Simplifies further **data cleaning steps**.  \n",
    "- ⚠️ Be careful with acronyms & proper nouns (`US` ≠ `us`).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa066184-31ec-4a8f-a65c-4b74afc25872",
   "metadata": {},
   "source": [
    "# 📝 Removing Stopwords with NLTK\n",
    "\n",
    "## 🚀 Introduction to Removing Stopwords\n",
    "In this lesson, we will use the **NLTK** package to remove stopwords from our text.\n",
    "\n",
    "👉 Stopwords are common words in the language that do not carry much meaning.  \n",
    "Examples: **\"and,\" \"of,\" \"a,\" \"to\"**  \n",
    "\n",
    "Removing stopwords helps by:\n",
    "- ⚡ Reducing complexity in the dataset  \n",
    "- 🎯 Improving machine learning accuracy  \n",
    "- ⏩ Speeding up processing time  \n",
    "\n",
    "---\n",
    "\n",
    "## 📥 Importing NLTK and Downloading Stopwords\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "If you have not already downloaded these, it may take a few minutes.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Assigning and Printing Stopwords\n",
    "\n",
    "```python\n",
    "n_stopwords = stopwords.words('english')\n",
    "print(n_stopwords)\n",
    "```\n",
    "\n",
    "✅ This gives you a list of common English stopwords.  \n",
    "\n",
    "---\n",
    "\n",
    "## ✂️ Removing Stopwords from a Sentence\n",
    "\n",
    "Sentence to process:\n",
    "```python\n",
    "sentence = 'It was too far to go to the shop and he did not want her to walk.'\n",
    "```\n",
    "\n",
    "Remove stopwords:\n",
    "```python\n",
    "sentence_no_stopwords = ' '.join([word for word in sentence.split() if word.lower() not in n_stopwords])\n",
    "print(sentence_no_stopwords)\n",
    "```\n",
    "\n",
    "📌 Output:\n",
    "```\n",
    "far go shop. Want walk.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Customizing the Stopwords List\n",
    "\n",
    "You can modify the list:  \n",
    "\n",
    "```python\n",
    "n_stopwords.remove('did')\n",
    "n_stopwords.remove('not')\n",
    "n_stopwords.append('go')\n",
    "\n",
    "sentence_no_stopwords_custom = ' '.join([word for word in sentence.split() if word.lower() not in n_stopwords])\n",
    "print(sentence_no_stopwords_custom)\n",
    "```\n",
    "\n",
    "📌 Output:\n",
    "```\n",
    "far shop, did not want walk\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Takeaways\n",
    "\n",
    "- 🛠️ **NLTK** provides built-in stopwords lists for many languages.  \n",
    "- 🧹 Removing stopwords simplifies text data and speeds up processing.  \n",
    "- 🎯 Cleaner datasets often improve ML model performance.  \n",
    "- ✍️ You can customize stopword lists by adding or removing words.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7df23-e1b7-483d-b851-35b56d612e24",
   "metadata": {},
   "source": [
    "# Regular Expressions in NLP\n",
    "\n",
    "## 📖 Introduction to Regular Expressions\n",
    "Regular expressions, or **regex**, are a special syntax for searching strings that match specified patterns.  \n",
    "They are a powerful tool to filter and sort through text when you want to **match patterns** instead of exact strings.  \n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Importing the `re` Package\n",
    "```python\n",
    "import re\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 Raw Strings in Python\n",
    "Python treats characters like `\\n` as special (newline).  \n",
    "To avoid misinterpretation, prefix strings with `r` to indicate a **raw string**.\n",
    "\n",
    "```python\n",
    "my_folder = \"c\\desktop\\notes\"   # Interprets \n",
    " as newline ❌\n",
    "print(my_folder)\n",
    "\n",
    "my_folder_raw = r\"c\\desktop\\notes\"  # Treated literally ✅\n",
    "print(my_folder_raw)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Regex Functions Overview\n",
    "\n",
    "- **`re.search(pattern, string)`** → returns match if found, else `None`  \n",
    "- **`re.sub(pattern, replacement, string)`** → replaces matched text  \n",
    "\n",
    "```python\n",
    "text = \"Sara was able to help me quickly.\"\n",
    "new_text = re.sub(\"Sara\", \"Sarah\", text)\n",
    "print(new_text)  # Sarah was able to help me quickly.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Regex Syntax Examples\n",
    "\n",
    "### ✅ Matching Optional Characters with `?`\n",
    "```python\n",
    "reviews = [\"Sara helped a lot.\", \"Sarah was kind.\"]\n",
    "pattern = r\"Sara?h\"\n",
    "for review in reviews:\n",
    "    if re.search(pattern, review):\n",
    "        print(review)\n",
    "# Matches both Sara and Sarah\n",
    "```\n",
    "\n",
    "### ✅ Start of String `^`\n",
    "```python\n",
    "re.search(r\"^A\", \"Amazing work!\")  # Matches\n",
    "```\n",
    "\n",
    "### ✅ End of String `$`\n",
    "```python\n",
    "re.search(r\"y$\", \"Great delivery\")  # Matches 'y' at end\n",
    "```\n",
    "\n",
    "### ✅ Alternation `|`\n",
    "```python\n",
    "pattern = r\"(need|want)ed\"\n",
    "text = \"I needed help and she wanted answers.\"\n",
    "print(re.findall(pattern, text))  # ['need', 'want']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✂️ Removing Punctuation\n",
    "```python\n",
    "reviews = [\"Amazing work!\", \"Really helpful :)\"]\n",
    "cleaned = [re.sub(r\"[^\\w\\s]\", \"\", review) for review in reviews]\n",
    "print(cleaned)  # ['Amazing work', 'Really helpful ']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎨 Regex Cheat Sheet (Visual)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A(^ = Start of String) --> B($ = End of String)\n",
    "    B --> C(? = Optional Character)\n",
    "    C --> D(| = OR / Alternation)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Key Takeaways\n",
    "- Regex provides **pattern-based searching** instead of raw string matching.  \n",
    "- Use `r\"\"` raw strings to avoid escape issues.  \n",
    "- Common functions: `re.search`, `re.sub`.  \n",
    "- Syntax:  \n",
    "  - `?` → optional character  \n",
    "  - `^` → start of string  \n",
    "  - `$` → end of string  \n",
    "  - `|` → alternation  \n",
    "- Regex is essential for **text preprocessing** (e.g., punctuation removal).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80b393-e99b-4478-b5ab-6dfd9f183841",
   "metadata": {},
   "source": [
    "# 📝 Tokenization\n",
    "\n",
    "## 🔹 Introduction\n",
    "A fundamental step in **Natural Language Processing (NLP)** involves converting our text into smaller units through a process known as **tokenization**.  \n",
    "These smaller units are called **tokens**.\n",
    "\n",
    "- **Word Tokenization** → Breaks text into words.  \n",
    "- **Sentence Tokenization** → Breaks text into sentences.  \n",
    "- Tokens can also be **subwords** or **characters**, depending on the use case.\n",
    "\n",
    "👉 We perform tokenization because the meaning of text is better understood if we can analyze the **individual parts** as well as the **whole**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ✂️ Sentence Tokenization Example\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Example text\n",
    "text = \"Her cat's name is Luna. Her dog's name is Max.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n",
    "```\n",
    "\n",
    "### ✅ Expected Output\n",
    "```python\n",
    "[\"Her cat's name is Luna.\", \"Her dog's name is Max.\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔤 Word Tokenization Example\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word tokenization on a single sentence\n",
    "sentence = \"Her cat's name is Luna.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(words)\n",
    "```\n",
    "\n",
    "### ✅ Expected Output\n",
    "```python\n",
    "['Her', 'cat', \"'s\", 'name', 'is', 'Luna', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Combined Tokenization Example\n",
    "```python\n",
    "# Word tokenization on the full text\n",
    "words_full = word_tokenize(text)\n",
    "print(words_full)\n",
    "\n",
    "# Convert all tokens to lowercase for consistency\n",
    "words_lower = [w.lower() for w in words_full]\n",
    "print(words_lower)\n",
    "```\n",
    "\n",
    "### ✅ Expected Output\n",
    "```python\n",
    "['Her', 'cat', \"'s\", 'name', 'is', 'Luna', '.', 'Her', 'dog', \"'s\", 'name', 'is', 'Max', '.']\n",
    "\n",
    "['her', 'cat', \"'s\", 'name', 'is', 'luna', '.', 'her', 'dog', \"'s\", 'name', 'is', 'max', '.']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Takeaways\n",
    "- Tokenization = splitting text into smaller units (**tokens**).  \n",
    "- Sentence tokenization → breaks text into **sentences**.  \n",
    "- Word tokenization → breaks text into **words**.  \n",
    "- Lowercasing tokens ensures consistency when analyzing frequencies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c319d-cc23-4863-8089-3200c4ed60ae",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 🔄 Stemming in NLP\n",
    "\n",
    "Stemming is the process of reducing words to their **base or root form**.  \n",
    "It is part of **text standardization** during preprocessing.\n",
    "\n",
    "For example:  \n",
    "- **connecting → connect**  \n",
    "- **connected → connect**  \n",
    "- **connectivity → connect**  \n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 Why Use Stemming?\n",
    "- Reduces the **number of unique words** in the dataset.  \n",
    "- Simplifies and lowers the **complexity** of data.  \n",
    "- Makes models focus on the **core meaning** instead of variations.  \n",
    "\n",
    "> [!IMPORTANT]  \n",
    "> Sometimes stemming produces non-words (e.g., *worse → wos*).  \n",
    "> Always balance simplicity vs. semantic accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Using NLTK’s Porter Stemmer\n",
    "\n",
    "### Example 1: Words around **connect**\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "PPS = PorterStemmer()\n",
    "\n",
    "tokens_connect = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "for token in tokens_connect:\n",
    "    print(token, \"→\", PPS.stem(token))\n",
    "```\n",
    "\n",
    "✅ Output:\n",
    "```\n",
    "connecting → connect\n",
    "connected → connect\n",
    "connectivity → connect\n",
    "connect → connect\n",
    "connects → connect\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Words around **learn**\n",
    "```python\n",
    "tokens_learn = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "for token in tokens_learn:\n",
    "    print(token, \"→\", PPS.stem(token))\n",
    "```\n",
    "\n",
    "✅ Output:\n",
    "```\n",
    "learned → learn\n",
    "learning → learn\n",
    "learn → learn\n",
    "learns → learn\n",
    "learner → learner\n",
    "learners → learner\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Irregular cases\n",
    "```python\n",
    "tokens_misc = ['likes', 'better', 'worse']\n",
    "for token in tokens_misc:\n",
    "    print(token, \"→\", PPS.stem(token))\n",
    "```\n",
    "\n",
    "✅ Output:\n",
    "```\n",
    "likes → like\n",
    "better → better\n",
    "worse → wors\n",
    "```\n",
    "\n",
    "> [!NOTE]  \n",
    "> Notice how *worse* is stemmed to *wors*. This shows that stemming is **rule-based**, not meaning-based.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Stemming Process Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Original Words] --> B[Stemming Rules]\n",
    "    B --> C[Base Form Tokens]\n",
    "    \n",
    "    A:::aStyle\n",
    "    B:::bStyle\n",
    "    C:::cStyle\n",
    "\n",
    "    classDef aStyle fill:#FFDDC1,stroke:#333,stroke-width:2px;\n",
    "    classDef bStyle fill:#C1E1FF,stroke:#333,stroke-width:2px;\n",
    "    classDef cStyle fill:#C1FFD7,stroke:#333,stroke-width:2px;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Takeaways\n",
    "- 🔡 **Stemming** reduces words to their base/root form by chopping suffixes.  \n",
    "- 📉 Reduces dataset size and complexity.  \n",
    "- ⚡ **Porter Stemmer** is widely used in Python via NLTK.  \n",
    "- ⚠️ Not always linguistically correct (produces non-words).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5bb3c5-6ce5-4eef-bd5c-b0cbfc4f9f4e",
   "metadata": {},
   "source": [
    "# 📝 Lemmatization in NLP\n",
    "\n",
    "## 📖 Introduction\n",
    "Where **stemming** removes the last few characters of a word, **lemmatization** reduces words to a more meaningful base form and ensures they do not lose their meaning.\n",
    "\n",
    "🔑 **Key Difference:**  \n",
    "- **Stemming**: Cuts off word endings (may result in non-meaningful roots).  \n",
    "- **Lemmatization**: Uses a predefined **dictionary (WordNet)** and **context** to ensure the base form is meaningful.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ How Lemmatization Works\n",
    "- Lemmatization works more intelligently by referencing a **predefined dictionary** containing the context of the word.  \n",
    "- Words are reduced only if their meaningful lemma exists.  \n",
    "- ✅ More accurate than stemming, but results in a larger dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 🐍 Python Example – Using NLTK\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet if not already\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example tokens\n",
    "tokens_connect = ['connecting', 'connected', 'connectivity', 'connect', 'connects']\n",
    "tokens_learn = ['learned', 'learning', 'learn', 'learns', 'learner', 'learners']\n",
    "tokens_likes = ['likes', 'better', 'worse']\n",
    "\n",
    "print(\"📌 Connect tokens:\")\n",
    "for token in tokens_connect:\n",
    "    print(f\"{token} ➝ {lemmatizer.lemmatize(token)}\")\n",
    "\n",
    "print(\"\\n📌 Learn tokens:\")\n",
    "for token in tokens_learn:\n",
    "    print(f\"{token} ➝ {lemmatizer.lemmatize(token)}\")\n",
    "\n",
    "print(\"\\n📌 Likes tokens:\")\n",
    "for token in tokens_likes:\n",
    "    print(f\"{token} ➝ {lemmatizer.lemmatize(token)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Example Output\n",
    "\n",
    "```\n",
    "📌 Connect tokens:\n",
    "connecting ➝ connecting\n",
    "connected ➝ connected\n",
    "connectivity ➝ connectivity\n",
    "connect ➝ connect\n",
    "connects ➝ connects\n",
    "\n",
    "📌 Learn tokens:\n",
    "learned ➝ learned\n",
    "learning ➝ learning\n",
    "learn ➝ learn\n",
    "learns ➝ learns\n",
    "learner ➝ learner\n",
    "learners ➝ learner\n",
    "\n",
    "📌 Likes tokens:\n",
    "likes ➝ like\n",
    "better ➝ better\n",
    "worse ➝ worse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Stemming vs Lemmatization\n",
    "\n",
    "| Feature               | Stemming | Lemmatization |\n",
    "|-----------------------|----------|---------------|\n",
    "| Uses dictionary?      | ❌ No    | ✅ Yes |\n",
    "| Speed                 | ⚡ Faster | 🐢 Slower |\n",
    "| Accuracy              | ❌ May produce non-words (e.g., *worse → wor*) | ✅ Produces meaningful words (e.g., *worse → worse*) |\n",
    "| Data size reduction   | ✅ Strong | ❌ Weak |\n",
    "| Preserves meaning     | ❌ Not always | ✅ Yes |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Simple Diagram\n",
    "\n",
    "```\n",
    "   Words → [Stemming] → \"wors\"\n",
    "        ↘ [Lemmatization] → \"worse\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🏁 Key Takeaways\n",
    "- ✂️ **Stemming** removes suffixes, sometimes breaking word meaning.  \n",
    "- 📚 **Lemmatization** reduces words to valid base forms using a dictionary.  \n",
    "- 📊 Lemmatization preserves **semantic meaning** but increases dataset size.  \n",
    "- ⚡ Use **stemming** when speed matters, **lemmatization** when accuracy matters.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e36fa6-fa49-49a8-b91f-d2e52dc351e9",
   "metadata": {},
   "source": [
    "# 📚 N-grams in NLP\n",
    "N-grams help us **inspect preprocessing**, **explore data**, and **engineer features** for ML.  \n",
    "An **n-gram** is a sequence of **n neighboring tokens** (words, subwords, or chars).\n",
    "\n",
    "> [!TIP]\n",
    "> Use n-grams after basic cleaning (lowercase, tokenization, stopword handling) to get clearer signal.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 Requirements\n",
    "```python\n",
    "# Core packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "# (no output on import)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Example Tokens (preprocessed-lite)\n",
    "These are sample tokens (lowercased, light cleaning) for demonstration:\n",
    "\n",
    "```python\n",
    "tokens = [\n",
    "    'two','of','the','cats','that','were','here',\n",
    "    'two','of','the','dogs','that','were','barking',\n",
    "    'two','of','the','birds','that','were','singing',\n",
    "    'two','of','them','were','happy'\n",
    "]\n",
    "print(tokens[:12])\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "['two', 'of', 'the', 'cats', 'that', 'were', 'here', 'two', 'of', 'the', 'dogs', 'that']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1️⃣ Unigrams (n = 1)\n",
    "\n",
    "```python\n",
    "# Count unigrams\n",
    "uni_counts = Counter(tokens)\n",
    "# Or via pandas for convenience\n",
    "uni_series = pd.Series(tokens).value_counts()\n",
    "\n",
    "print(\"Top unigrams (Counter):\", uni_counts.most_common(5))\n",
    "print(\"\\nTop unigrams (pandas):\\n\", uni_series.head(10))\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "Top unigrams (Counter): [('two', 4), ('of', 4), ('were', 4), ('the', 3), ('that', 3)]\n",
    "\n",
    "Top unigrams (pandas):\n",
    "two      4\n",
    "of       4\n",
    "were     4\n",
    "the      3\n",
    "that     3\n",
    "cats     1\n",
    "here     1\n",
    "dogs     1\n",
    "barking  1\n",
    "birds    1\n",
    "dtype: int64\n",
    "```\n",
    "\n",
    "### 📊 Visualize Top-10 Unigrams\n",
    "> [!NOTE]\n",
    "> Plotting uses **matplotlib** (single chart, default colors).\n",
    "\n",
    "```python\n",
    "top10 = uni_series.head(10).sort_values()\n",
    "plt.figure(figsize=(8, 4))\n",
    "top10.plot.barh()\n",
    "plt.title(\"Top 10 Unigrams\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "# A horizontal bar chart is displayed.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2️⃣ Bigrams (n = 2)\n",
    "```python\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "bigram_counts = Counter(bigrams)\n",
    "print(bigram_counts.most_common(5))\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "[(('two', 'of'), 4), (('of', 'the'), 3), (('that', 'were'), 3), (('the', 'cats'), 1), (('cats', 'that'), 1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3️⃣ Trigrams (n = 3)\n",
    "```python\n",
    "trigrams = list(ngrams(tokens, 3))\n",
    "trigram_counts = Counter(trigrams)\n",
    "print(trigram_counts.most_common(5))\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "[(('two', 'of', 'the'), 3), (('of', 'the', 'cats'), 1), (('the', 'cats', 'that'), 1), (('cats', 'that', 'were'), 1), (('that', 'were', 'here'), 1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Quick Helper: N-gram Frequencies\n",
    "```python\n",
    "def ngram_freq(tokens, n=2, top_k=10):\n",
    "    counts = Counter(ngrams(tokens, n))\n",
    "    return counts.most_common(top_k)\n",
    "\n",
    "print(\"Top-10 bigrams:\", ngram_freq(tokens, n=2, top_k=10))\n",
    "print(\"Top-10 trigrams:\", ngram_freq(tokens, n=3, top_k=10))\n",
    "```\n",
    "✅ Output:\n",
    "```\n",
    "Top-10 bigrams: [(('two', 'of'), 4), (('of', 'the'), 3), (('that', 'were'), 3), (('the', 'cats'), 1), (('cats', 'that'), 1), (('the', 'dogs'), 1), (('dogs', 'that'), 1), (('were', 'barking'), 1), (('the', 'birds'), 1), (('birds', 'that'), 1)]\n",
    "Top-10 trigrams: [(('two', 'of', 'the'), 3), (('of', 'the', 'cats'), 1), (('the', 'cats', 'that'), 1), (('cats', 'that', 'were'), 1), (('that', 'were', 'here'), 1), (('of', 'the', 'dogs'), 1), (('the', 'dogs', 'that'), 1), (('dogs', 'that', 'were'), 1), (('that', 'were', 'barking'), 1), (('of', 'the', 'birds'), 1)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 How N-grams Slide (Diagram)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A1[\"Tokens: two → of → the → cats → that → were → ...\"]\n",
    "    A2[\"Window (n=2): [two of] → [of the] → [the cats] → ...\"]\n",
    "    A3[\"Window (n=3): [two of the] → [of the cats] → [the cats that] → ...\"]\n",
    "    A1 --> A2 --> A3\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Key Takeaways\n",
    "- An **n-gram** is a sequence of **n neighboring tokens**.  \n",
    "- **Unigrams, bigrams, trigrams** correspond to **n = 1, 2, 3**.  \n",
    "- With **NLTK + pandas**, you can **count** and **visualize** n-grams quickly.  \n",
    "- N-gram analysis becomes powerful after **thorough preprocessing** and on **larger corpora**.\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> For production features, consider:\n",
    "> - filtering stopwords\n",
    "> - normalizing (lowercase, stemming/lemmatization)\n",
    "> - min-frequency thresholds to reduce noise\n",
    "> - character n-grams for languages without whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67b48b5-1e33-4a67-8b02-d8c135e07361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
